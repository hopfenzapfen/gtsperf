\section{Conclusion} \label{conclusion}
%Docker containers and container-based solutions in general are challenging virtual machines in cloud environments. An increasing number of companies are looking to either complement or even completely replace their virtual compute infrastructure with containers. Containers are lightweight and allow for a high density of deployed services. This transition is eased with the introduction of \texttt{libnetwork} in Docker, which allows for connecting geographically dispersed containers via means of either a native or a third party overlay network.

In this paper, we have evaluated the performance of various Docker overlay solutions in a high latency environment, and more specifically, in the GÃ‰ANT Testbeds Service. We assessed the native overlay driver included in \texttt{libnetwork} and third party solutions Flannel and Weave by means of a synthetic point-to-point benchmark and a streaming media application benchmark in a star topology. During the project, Calico was found to be infeasible to deploy in the GTS. We saw that the remaining solutions are very similar on a technical level and employ similar VXLAN tunneling techniques to connect containers. 
\\
\\
Our results indicate that the performance of the overlay solutions with regards to latency as well as jitter is very similar. The level of overhead introduced by the overlay can be considered almost negligible (in the range of 0.1 ms). The same behavior was found whilst measuring the performance of the overlays in the streaming media benchmark. All overlays exhibit similar performance, regardless of the amount of locations in the topology or clients requesting a stream. As such, we conclude that the native overlay driver performs equal to the evaluated third party solutions. Our point-to-point measurements show irregular behavior with regards to UDP and TCP throughput and require further investigation.
\\
\\
Taking all of the measurements into account, we conclude that geographic dispersion has no significant effect on either latency or jitter when evaluated in the GTS specifically. 

\subsection{Future work}
During the course of this project a GitHub repository\footnote{All created resources are available at \url{https://github.com/siemhermans/gtsperf}} has been maintained which is designed to make future deployments of the evaluated overlay solutions easier, particularly in GTS. However, although GTS is an excellent environment to test the synthetic performance of the overlays, it would be interesting to reconduct the performance analysis in a heavily shared environment (e.g. Amazon EC2 or Azure). This would require a slight modification of the provided scripts but would illustrate the effect of real world congestion on performance. Additionally, the measurements should be repeated in an environment which is less pressed for resources, in order to eliminate the CPU as a variable whilst measuring. With regards to the measured results, further investigation is required to identify the root cause of the anomalies seen in UDP and TCP throughput.  

With regards to the environment, the current measurements have been performed with a generic Infrastructure as a Service use case in mind. This means that the Docker containers have been deployed within virtual machines. In the future it would be interesting to see how the performance of the overlay solutions compares when the containers are deployed in a special container hypervisor like LXD \cite{ubuntulxd}. This would require a cloud provider to provide (near) bare metal provision resources. At this point in time, bare metal provisioning is not supported in GTS, however, it is a confirmed road-mapped functionality. Therefore, the GTS may potentially be used for this purpose in the future.
