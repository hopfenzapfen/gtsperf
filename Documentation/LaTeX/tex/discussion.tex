\section{Discussion} \label{discussion}
During the course of this project we measured the performance of various overlay solutions when implemented in a high latency environment. When starting out with the research, our hypothesis was that the overlay solutions would perform worse than the underlay in any given situation. This was based on the general notion that an overlay solution introduces an implicit performance overhead by adding another layer to the network. Additionally, as most overlay solutions now resort to in-kernel traffic handling, we did not expect the decrease in performance to be very large. 
\\\\
The results of the point-to-point UDP and TCP throughput measurements with \texttt{iperf} did not match our expectations. We saw irregular behavior whilst measuring TCP throughput. In most situations the overlay solution outperforms the underlay but sometimes the opposite is true. The measurements as presented in Appendix \ref{app:PtP_through} indicate that this behavior is not specific to an individual circuit. Similarly, we expected a small deterioration in performance whilst measuring the UDP throughput between containers in the overlays, but not with a margin of circa 100 to 150 Mbps. Interestingly enough, our results indicate that the anomalies regarding UDP throughput are constant. Regardless of the tested circuit a significant performance drop is seen. 

We are not the first ones to find anomalies in UDP throughput measurements. Claassen examined similar behavior in his paper regarding Docker network performance in a local environment \cite{jorisclaassen2015}. He hypothesized that the deterioration could be related to the way \texttt{iperf} functions with regards to its utilization of CPU cycles whilst measuring. Regarding the disappointing UDP throughput measured with \texttt{iperf}, Claassen's hypothesis holds. The fact that the streaming media tests also utilize UDP and do not show such losses does support the notion that this is indeed an \texttt{iperf} specific problem. Nevertheless, this does not explain why the overlay solutions outperform the underlay when TCP throughput is measured, which leads us to believe that this issue is specific to the GÃ‰ANT Testbeds Service. Further investigation is required.
\\\\
Due to the irregular results, we will presume the measurements in the point-to-point topology regarding UDP and TCP throughput are unreliable and as such, will not be used as a basis to form a conclusion. This decision does not impact the latency measurements however. These measurements consistently show that the underlay outperforms the overlay by a very small margin (below 1 millisecond). 
\\\\
With regards to latency, surprisingly enough, the latency measured within GTS resembled that of a non-shared environment. This is likely the case due to the fact that the environment is not intensively being used by other researchers. Because of this, we have been able to get unadulterated results regarding the performance of these overlays. As a consequence we have not experienced a level of congestion which would be expected in a real world scenario. It is possible that performing the experiments in an environment with a high(er) level of congestion would yield different results. This is especially true for the streaming media tests as in our current experiment we have only been able to see the effect of artificial congestion. 
\\\\
During the feasibility-check of the overlay solutions, we have noticed a difference in ease of setup between the different overlay technologies. Weave has proven to be the most inclusive package requiring no additional software to be installed while the other overlays require at least a separate key-value store. Considering there are no significant performance differences observed between the overlays -with regards to latency-, Weave could be considered the preferred solution within this specific environment. However, we feel that the choice for a certain solution is very specific to the use case of the network. For example, due to the fast deployment capabilities of Weave, the solution lends itself especially well for rapid prototyping. Flannel on the other hand is built to specifically to integrate with Google's Kubernetes orchestration tool which allows for the administration of very large clusters. Calico pursues a different model and is mainly aimed at high performance data center deployments. As a closing remark we think that the integration a solution offers with third party tools will be a hugely deciding factor when selecting a solution, as all of the solutions pursue a similar goal: interconnecting containers dispersed over multiple hosts, regardless of their physical location. 